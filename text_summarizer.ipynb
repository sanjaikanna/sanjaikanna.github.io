{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAtDsxLvSXUAm/LBkcstyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjaikanna/sanjaikanna.github.io/blob/main/text_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from google.colab import files\n",
        "\n",
        "# Downloading required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.punctuation = set(string.punctuation)\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"Custom text processing pipeline\"\"\"\n",
        "        # Store original sentences\n",
        "        self.original_sentences = sent_tokenize(text)\n",
        "\n",
        "        # Word-level preprocessing\n",
        "        processed_sentences = []\n",
        "        for sent in self.original_sentences:\n",
        "            words = nltk.word_tokenize(sent.lower())\n",
        "            words = [w for w in words if w not in self.stop_words and w not in self.punctuation]\n",
        "            processed_sentences.append(' '.join(words))\n",
        "\n",
        "        return processed_sentences\n",
        "\n",
        "    def train_model(self, processed_sentences):\n",
        "        \"\"\"Train TF-IDF model on the processed text\"\"\"\n",
        "        return self.vectorizer.fit_transform(processed_sentences)\n",
        "\n",
        "    def generate_summary(self, text, num_sentences=3):\n",
        "        \"\"\"End-to-end summarization with custom trained model\"\"\"\n",
        "        processed_sentences = self.preprocess(text)\n",
        "        X = self.train_model(processed_sentences)\n",
        "\n",
        "        # Converting it to array and calculate scores\n",
        "        sentence_scores = np.array(X.sum(axis=1)).flatten()\n",
        "        top_sentence_indices = np.argsort(-sentence_scores)[:num_sentences]\n",
        "        top_sentence_indices.sort()  # Maintaining original order\n",
        "\n",
        "        return ' '.join([self.original_sentences[i] for i in top_sentence_indices])\n",
        "\n",
        "def main():\n",
        "    print(\"ðŸ§  AI-Powered Text Summarizer (Custom Implementation)\")\n",
        "    print(\"1. Paste text\\n2. Upload file\\n\")\n",
        "\n",
        "    choice = input(\"Choose input method (1/2): \").strip()\n",
        "    text = \"\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"\\nPaste your text (press Enter twice to finish):\")\n",
        "        lines = []\n",
        "        while True:\n",
        "            line = input()\n",
        "            if not line and lines:\n",
        "                break\n",
        "            lines.append(line)\n",
        "        text = \"\\n\".join(lines)\n",
        "    elif choice == \"2\":\n",
        "        print(\"\\nUpload text file:\")\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            text = next(iter(uploaded.values())).decode('utf-8')\n",
        "\n",
        "    if not text.strip():\n",
        "        print(\"Error: No text provided!\")\n",
        "        return\n",
        "\n",
        "    # Initializing and using custom summarizer\n",
        "    summarizer = TextSummarizer()\n",
        "    summary = summarizer.generate_summary(text)\n",
        "\n",
        "    print(\"\\n=== ORIGINAL TEXT ===\")\n",
        "    print(text[:500] + (\"...\" if len(text) > 500 else \"\"))\n",
        "    print(\"\\n=== AI-GENERATED SUMMARY ===\")\n",
        "    print(summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KX6mmr1Zyuq",
        "outputId": "59a95d00-3d79-4f43-9563-76f4c4951dc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§  AI-Powered Text Summarizer (Custom Implementation)\n",
            "1. Paste text\n",
            "2. Upload file\n",
            "\n",
            "Choose input method (1/2): 1\n",
            "\n",
            "Paste your text (press Enter twice to finish):\n",
            "The most notable thing about Time is that it is so purely relative. A large amount of reminiscence is, by common consent, conceded to the drowning man; and it is not past belief that one may review an entire courtship while removing one's gloves. That is what Trysdale was doing, standing by a table in his bachelor apartments. On the table stood a singular-looking green plant in a red earthen jar. The plant was one of the species of cacti, and was provided with long, tentacular leaves that perpetually swayed with the slightest breeze with a peculiar creeping motion almost sentient\n",
            "\n",
            "\n",
            "=== ORIGINAL TEXT ===\n",
            "The most notable thing about Time is that it is so purely relative. A large amount of reminiscence is, by common consent, conceded to the drowning man; and it is not past belief that one may review an entire courtship while removing one's gloves. That is what Trysdale was doing, standing by a table in his bachelor apartments. On the table stood a singular-looking green plant in a red earthen jar. The plant was one of the species of cacti, and was provided with long, tentacular leaves that perpet...\n",
            "\n",
            "=== AI-GENERATED SUMMARY ===\n",
            "A large amount of reminiscence is, by common consent, conceded to the drowning man; and it is not past belief that one may review an entire courtship while removing one's gloves. On the table stood a singular-looking green plant in a red earthen jar. The plant was one of the species of cacti, and was provided with long, tentacular leaves that perpetually swayed with the slightest breeze with a peculiar creeping motion almost sentient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.punctuation = set(string.punctuation)\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        self.original_sentences = sent_tokenize(text)\n",
        "        processed_sentences = []\n",
        "        for sent in self.original_sentences:\n",
        "            words = nltk.word_tokenize(sent.lower())\n",
        "            words = [w for w in words if w not in self.stop_words and w not in self.punctuation]\n",
        "            processed_sentences.append(' '.join(words))\n",
        "        return processed_sentences\n",
        "\n",
        "    def generate_summary(self, text, num_sentences=3):\n",
        "        processed_sentences = self.preprocess(text)\n",
        "        X = self.vectorizer.fit_transform(processed_sentences)\n",
        "        sentence_scores = np.array(X.sum(axis=1)).flatten()\n",
        "        top_sentence_indices = np.argsort(-sentence_scores)[:num_sentences]\n",
        "        top_sentence_indices.sort()\n",
        "        return ' '.join([self.original_sentences[i] for i in top_sentence_indices])\n",
        "\n",
        "@app.route('/summarize', methods=['POST'])\n",
        "def summarize():\n",
        "    data = request.get_json()\n",
        "    text = data.get('text', '')\n",
        "    num_sentences = data.get('num_sentences', 3)\n",
        "\n",
        "    if not text.strip():\n",
        "        return jsonify({'error': 'No text provided'}), 400\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "    summary = summarizer.generate_summary(text, num_sentences)\n",
        "\n",
        "    return jsonify({\n",
        "        'original_text': text,\n",
        "        'summary': summary,\n",
        "        'num_sentences': num_sentences\n",
        "    })\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GHkVQ1viMv5",
        "outputId": "3e2671fb-f82f-4e01-acf6-5b14feeb1d54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls  # checking whether it is store in app.py or not\n",
        "!cat app.py  # View its contents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFMNstnorp3i",
        "outputId": "83e58ad7-fe7b-42a2-949e-9838db84af86"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tsample_data  the-cactus.txt\n",
            "from flask import Flask, request, jsonify\n",
            "import nltk\n",
            "import numpy as np\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from nltk.tokenize import sent_tokenize\n",
            "from nltk.corpus import stopwords\n",
            "import string\n",
            "\n",
            "nltk.download('punkt')\n",
            "nltk.download('stopwords')\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "class TextSummarizer:\n",
            "    def __init__(self):\n",
            "        self.stop_words = set(stopwords.words('english'))\n",
            "        self.punctuation = set(string.punctuation)\n",
            "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
            "    \n",
            "    def preprocess(self, text):\n",
            "        self.original_sentences = sent_tokenize(text)\n",
            "        processed_sentences = []\n",
            "        for sent in self.original_sentences:\n",
            "            words = nltk.word_tokenize(sent.lower())\n",
            "            words = [w for w in words if w not in self.stop_words and w not in self.punctuation]\n",
            "            processed_sentences.append(' '.join(words))\n",
            "        return processed_sentences\n",
            "    \n",
            "    def generate_summary(self, text, num_sentences=3):\n",
            "        processed_sentences = self.preprocess(text)\n",
            "        X = self.vectorizer.fit_transform(processed_sentences)\n",
            "        sentence_scores = np.array(X.sum(axis=1)).flatten()\n",
            "        top_sentence_indices = np.argsort(-sentence_scores)[:num_sentences]\n",
            "        top_sentence_indices.sort()\n",
            "        return ' '.join([self.original_sentences[i] for i in top_sentence_indices])\n",
            "\n",
            "@app.route('/summarize', methods=['POST'])\n",
            "def summarize():\n",
            "    data = request.get_json()\n",
            "    text = data.get('text', '')\n",
            "    num_sentences = data.get('num_sentences', 3)\n",
            "    \n",
            "    if not text.strip():\n",
            "        return jsonify({'error': 'No text provided'}), 400\n",
            "    \n",
            "    summarizer = TextSummarizer()\n",
            "    summary = summarizer.generate_summary(text, num_sentences)\n",
            "    \n",
            "    return jsonify({\n",
            "        'original_text': text,\n",
            "        'summary': summary,\n",
            "        'num_sentences': num_sentences\n",
            "    })\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(host='0.0.0.0', port=5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch app.py  # Creating empty file\n",
        "!echo \"from flask import Flask\" >> app.py  # Adding content line by line\n",
        "!echo \"app = Flask(__name__)\" >> app.py\n",
        "!echo \"@app.route('/')\" >> app.py\n",
        "!echo \"def hello(): return 'Hello World!'\" >> app.py\n",
        "!cat app.py  # Verifying"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as6TaTmTruQo",
        "outputId": "57b44a7b-563c-4067-cbd8-9c1c293e641e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from flask import Flask, request, jsonify\n",
            "import nltk\n",
            "import numpy as np\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from nltk.tokenize import sent_tokenize\n",
            "from nltk.corpus import stopwords\n",
            "import string\n",
            "\n",
            "nltk.download('punkt')\n",
            "nltk.download('stopwords')\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "class TextSummarizer:\n",
            "    def __init__(self):\n",
            "        self.stop_words = set(stopwords.words('english'))\n",
            "        self.punctuation = set(string.punctuation)\n",
            "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
            "    \n",
            "    def preprocess(self, text):\n",
            "        self.original_sentences = sent_tokenize(text)\n",
            "        processed_sentences = []\n",
            "        for sent in self.original_sentences:\n",
            "            words = nltk.word_tokenize(sent.lower())\n",
            "            words = [w for w in words if w not in self.stop_words and w not in self.punctuation]\n",
            "            processed_sentences.append(' '.join(words))\n",
            "        return processed_sentences\n",
            "    \n",
            "    def generate_summary(self, text, num_sentences=3):\n",
            "        processed_sentences = self.preprocess(text)\n",
            "        X = self.vectorizer.fit_transform(processed_sentences)\n",
            "        sentence_scores = np.array(X.sum(axis=1)).flatten()\n",
            "        top_sentence_indices = np.argsort(-sentence_scores)[:num_sentences]\n",
            "        top_sentence_indices.sort()\n",
            "        return ' '.join([self.original_sentences[i] for i in top_sentence_indices])\n",
            "\n",
            "@app.route('/summarize', methods=['POST'])\n",
            "def summarize():\n",
            "    data = request.get_json()\n",
            "    text = data.get('text', '')\n",
            "    num_sentences = data.get('num_sentences', 3)\n",
            "    \n",
            "    if not text.strip():\n",
            "        return jsonify({'error': 'No text provided'}), 400\n",
            "    \n",
            "    summarizer = TextSummarizer()\n",
            "    summary = summarizer.generate_summary(text, num_sentences)\n",
            "    \n",
            "    return jsonify({\n",
            "        'original_text': text,\n",
            "        'summary': summary,\n",
            "        'num_sentences': num_sentences\n",
            "    })\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(host='0.0.0.0', port=5000)\n",
            "from flask import Flask\n",
            "app = Flask(__name__)\n",
            "@app.route('/')\n",
            "def hello(): return 'Hello World!'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm app.py"
      ],
      "metadata": {
        "id": "9X0wKHLYr13D"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, request, jsonify\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.punctuation = set(string.punctuation)\n",
        "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        self.original_sentences = sent_tokenize(text)\n",
        "        processed_sentences = []\n",
        "        for sent in self.original_sentences:\n",
        "            words = nltk.word_tokenize(sent.lower())\n",
        "            words = [w for w in words if w not in self.stop_words and w not in self.punctuation]\n",
        "            processed_sentences.append(' '.join(words))\n",
        "        return processed_sentences\n",
        "\n",
        "    def generate_summary(self, text, num_sentences=3):\n",
        "        processed_sentences = self.preprocess(text)\n",
        "        X = self.vectorizer.fit_transform(processed_sentences)\n",
        "        sentence_scores = np.array(X.sum(axis=1)).flatten()\n",
        "        top_sentence_indices = np.argsort(-sentence_scores)[:num_sentences]\n",
        "        top_sentence_indices.sort()\n",
        "        return ' '.join([self.original_sentences[i] for i in top_sentence_indices])\n",
        "\n",
        "@app.route('/summarize', methods=['POST'])\n",
        "def summarize():\n",
        "    data = request.get_json()\n",
        "    text = data.get('text', '')\n",
        "    num_sentences = data.get('num_sentences', 3)\n",
        "\n",
        "    if not text.strip():\n",
        "        return jsonify({'error': 'No text provided'}), 400\n",
        "\n",
        "    summarizer = TextSummarizer()\n",
        "    summary = summarizer.generate_summary(text, num_sentences)\n",
        "\n",
        "    return jsonify({\n",
        "        'original_text': text,\n",
        "        'summary': summary,\n",
        "        'num_sentences': num_sentences\n",
        "    })\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zqVkJXbuTQ0",
        "outputId": "6439df19-358c-4f1f-c7e2-24a3938fe4f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QDTIWgtuWbQ",
        "outputId": "3a22f400-417d-4f4b-aa46-c665d89fc4a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from flask import Flask, request, jsonify\n",
            "import nltk\n",
            "import numpy as np\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from nltk.tokenize import sent_tokenize\n",
            "from nltk.corpus import stopwords\n",
            "import string\n",
            "\n",
            "nltk.download('punkt')\n",
            "nltk.download('stopwords')\n",
            "\n",
            "app = Flask(__name__)\n",
            "\n",
            "class TextSummarizer:\n",
            "    def __init__(self):\n",
            "        self.stop_words = set(stopwords.words('english'))\n",
            "        self.punctuation = set(string.punctuation)\n",
            "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
            "    \n",
            "    def preprocess(self, text):\n",
            "        self.original_sentences = sent_tokenize(text)\n",
            "        processed_sentences = []\n",
            "        for sent in self.original_sentences:\n",
            "            words = nltk.word_tokenize(sent.lower())\n",
            "            words = [w for w in words if w not in self.stop_words and w not in self.punctuation]\n",
            "            processed_sentences.append(' '.join(words))\n",
            "        return processed_sentences\n",
            "    \n",
            "    def generate_summary(self, text, num_sentences=3):\n",
            "        processed_sentences = self.preprocess(text)\n",
            "        X = self.vectorizer.fit_transform(processed_sentences)\n",
            "        sentence_scores = np.array(X.sum(axis=1)).flatten()\n",
            "        top_sentence_indices = np.argsort(-sentence_scores)[:num_sentences]\n",
            "        top_sentence_indices.sort()\n",
            "        return ' '.join([self.original_sentences[i] for i in top_sentence_indices])\n",
            "\n",
            "@app.route('/summarize', methods=['POST'])\n",
            "def summarize():\n",
            "    data = request.get_json()\n",
            "    text = data.get('text', '')\n",
            "    num_sentences = data.get('num_sentences', 3)\n",
            "    \n",
            "    if not text.strip():\n",
            "        return jsonify({'error': 'No text provided'}), 400\n",
            "    \n",
            "    summarizer = TextSummarizer()\n",
            "    summary = summarizer.generate_summary(text, num_sentences)\n",
            "    \n",
            "    return jsonify({\n",
            "        'original_text': text,\n",
            "        'summary': summary,\n",
            "        'num_sentences': num_sentences\n",
            "    })\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    app.run(host='0.0.0.0', port=5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPBfFJXkuZ9y",
        "outputId": "0dca59f6-7e0c-42db-f8fe-9fe63ddea4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "text = \"\"\"The most notable thing about Time is that it is so purely relative. A large amount of reminiscence is, by common consent, conceded to the drowning man; and it is not past belief that one may review an entire courtship while removing one's gloves. That is what Trysdale was doing, standing by a table in his bachelor apartments. On the table stood a singular-looking green plant in a red earthen jar. The plant was one of the species of cacti, and was provided with long, tentacular leaves that perpetually swayed with the slightest breeze with a peculiar creeping motion almost sentient\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = requests.post(\n",
        "    \"http://localhost:5000/summarize\",\n",
        "    json={\"text\": text, \"num_sentences\": 2}\n",
        ")\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "IijvLlVPyZG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U13XTHRNy8a2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}